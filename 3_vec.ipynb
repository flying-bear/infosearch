{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "3 vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flying-bear/infosearch/blob/master/3_vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpQQIjH4SbYa",
        "colab_type": "text"
      },
      "source": [
        "let us install lemmatization for fasttext model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNBHQdqLbepf",
        "colab_type": "code",
        "outputId": "6479b6f8-3e30-4009-bed0-1cb582eba80b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install pymorphy2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.6/dist-packages (0.8)\n",
            "Requirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (2.4.393442.3710985)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0FKnf1oSgwM",
        "colab_type": "text"
      },
      "source": [
        "**IMPORTS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwN4nSzjYUq-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d7641b37-b82c-4a43-c464-9e4226a7a12a"
      },
      "source": [
        "%load_ext autoreload\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pymorphy2\n",
        "import re\n",
        "import sys\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from google.colab import drive\n",
        "from math import log\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from time import time\n",
        "\n",
        "trained_size = 2000"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hxrKwP08iT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def enum_sort(arr): \n",
        "  \"\"\"\n",
        "  sorts list by values and returns sorted ids\n",
        "  :param arr: list to be sorted\n",
        "  :return: list of ids sorted by thir values in arr\n",
        "  \"\"\"\n",
        "  return [x[0] for x in sorted(enumerate(arr), key=lambda x:x[1], reverse=True)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn7GDTgISlY8",
        "colab_type": "text"
      },
      "source": [
        "oh, well, colab stuff <br>\n",
        "\n",
        "(**I AM SO SORRY FOR ABSOLUTE PATHS BUT MY LAPTOP IS DEAD**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPFOLeF23Jpg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a121deb8-1db3-4376-81a4-5243c5dc7987"
      },
      "source": [
        "os.path.abspath('.')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_b50pqtYpkp",
        "colab_type": "code",
        "outputId": "8c14dffb-94b8-4923-e425-ea8df936d44a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6iEzNTCYtIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_path = 'gdrive/My Drive/studies/HSE/prog/infosearch/3 vec'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu9_TBS57we9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0b763c7-1d85-419d-aa68-84467ff74fa3"
      },
      "source": [
        "os.path.abspath('.')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msv4JMDPp3ml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "998a8e11-140d-456f-c570-2907728f223d"
      },
      "source": [
        "#shutil.copytree('gdrive/My Drive/studies/HSE/prog/infosearch/3 vec/elmo', 'content/elmo')\n",
        "#shutil.copytree(root_path+'/simple_elmo/bilm', 'content/bilm')\n",
        "#shutil.copy('gdrive/My Drive/studies/HSE/prog/infosearch/2 bm25/quora_question_pairs_rus.csv', 'content')\n",
        "#shutil.copytree(root_path+'/fasttext', 'content/fasttext')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'content/elmo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvtzdIA4S1nm",
        "colab_type": "text"
      },
      "source": [
        "**let's load the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMcRh8ITYUrI",
        "colab_type": "code",
        "outputId": "8bfe8b41-b341-432c-f3d1-2b30f7cd8475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "questions = pd.read_csv('content/quora_question_pairs_rus.csv', index_col=0).dropna()\n",
        "questions.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Какова история кохинор кох-и-ноор-бриллиант</td>\n",
              "      <td>что произойдет, если правительство Индии украд...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>как я могу увеличить скорость моего интернет-с...</td>\n",
              "      <td>как повысить скорость интернета путем взлома ч...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>почему я мысленно очень одинок, как я могу это...</td>\n",
              "      <td>найти остаток, когда математика 23 ^ 24 матема...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>которые растворяют в воде быстро сахарную соль...</td>\n",
              "      <td>какая рыба выживет в соленой воде</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>астрология: я - луна-колпачок из козерога и кр...</td>\n",
              "      <td>Я тройная луна-козерог и восхождение в козерог...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           question1  ... is_duplicate\n",
              "0        Какова история кохинор кох-и-ноор-бриллиант  ...            0\n",
              "1  как я могу увеличить скорость моего интернет-с...  ...            0\n",
              "2  почему я мысленно очень одинок, как я могу это...  ...            0\n",
              "3  которые растворяют в воде быстро сахарную соль...  ...            0\n",
              "4  астрология: я - луна-колпачок из козерога и кр...  ...            1\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMgE8BsbS50A",
        "colab_type": "text"
      },
      "source": [
        "**only some texts will be used, a part defined by trained_size constant above**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtbDYnUhzDWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_texts = questions[:trained_size]['question2'].dropna().tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy_3-ugES-9u",
        "colab_type": "text"
      },
      "source": [
        "**let's define preprocessing and lemmatiztion for fasttext**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0HUTx4HfsC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "morph=pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def strip_split(text): \n",
        "  \"\"\"\n",
        "  cleans a text of punctuation and case\n",
        "  \n",
        "  :param text: string to be cleaned\n",
        "  :return: a list of strings (words) in lowercase and stripped of punctuation\n",
        "  \"\"\"\n",
        "  low = text.lower()\n",
        "  stripped = re.sub('!|\\.|,|#|$|%|\\\\|\\'|\\(|\\)|-|\\+|\\*|/|\\:|;|<|>|=|\\?|\\[|\\]|@|^|_|`|{|}|~', '', low)\n",
        "  words = stripped.split()\n",
        "  return words\n",
        "\n",
        "def lemmatize(text):\n",
        "  \"\"\"\n",
        "  lemmatizes string using PyMorphy\n",
        "  \n",
        "  :param text: string to be lemmatized\n",
        "  :return: list of lemmatized words (strings)\n",
        "  \"\"\"\n",
        "  return [morph.parse(word)[0].normal_form for word in strip_split(text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4smWBr0YUrr",
        "colab_type": "text"
      },
      "source": [
        "## Задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXegxXWoYUrt",
        "colab_type": "text"
      },
      "source": [
        "Реализуйте поиск по [Quora question pairs](https://www.kaggle.com/loopdigga/quora-question-pairs-russian) на нескольких векторных моделях\n",
        "\n",
        "    1. fasttext, модель ruscorpora_none_fasttextskipgram_300_2_2019\n",
        "    2. elmo, модель ruwikiruscorpora_lemmas_elmo_1024_2019\n",
        "    3. bert*, RuBERT - необязательно\n",
        "   \n",
        "Первые две обученные модели можно скачать на сайте [rusvectores](https://rusvectores.org/en/models/).\n",
        "\n",
        "BERT делать необязательно, но если сделаете, 6 за курс у вас автоматом. Модель можно [найти тут](http://docs.deeppavlov.ai/en/master/features/models/bert.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvOaoNHddEQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cos_sim(v1, v2):\n",
        "  \"\"\"\n",
        "  cosine similarity of two vectors\n",
        "  \n",
        "  :param v1: np.array, first vector\n",
        "  :param v2: np.array, second vector\n",
        "  :return: float, cosine similarity of the vectors\n",
        "  \"\"\"\n",
        "  return np.inner(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6ym_Ea5YUru",
        "colab_type": "text"
      },
      "source": [
        "### __Задача 1__:    \n",
        "Сравните время индексации корпуса для каждой модели "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ldn02sJ_doe",
        "colab_type": "text"
      },
      "source": [
        "**FASTTEXT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v52CX85gX-jP",
        "colab_type": "text"
      },
      "source": [
        "load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5lFf6ikYUrQ",
        "colab_type": "code",
        "outputId": "6b1830fd-bd9a-4bfe-d635-b79593194ac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# если модель без тэгов\n",
        "model_file = '/'.join(['content', 'fasttext', 'model.model'])\n",
        "model = KeyedVectors.load(model_file)"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Oa1K4rAYUrZ",
        "colab_type": "text"
      },
      "source": [
        "проверка наличия слова в словаре"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCZnc9gLcDQr",
        "colab_type": "code",
        "outputId": "8dd8e525-e76d-46d1-9d2a-d9fb25643f8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lemmatize('Я полнейШий дебИл! Не умею ДУМАТЬ (головой)')"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['я', 'полный', 'дебил', 'не', 'уметь', 'думать', 'голова']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k95J7qVfYUra",
        "colab_type": "code",
        "outputId": "340819df-3966-41a8-8acf-b0bc0eeb8fa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('я' in model.vocab, 'черепаха' in model.vocab)"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHaIbMzDYUrg",
        "colab_type": "text"
      },
      "source": [
        "получение вектора слова"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LW1bbFrxYUri",
        "colab_type": "code",
        "outputId": "05b1a138-2661-43f3-ee36-63627b421adb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "model['черепаха'][10:20]"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.14401978, -0.34113285,  0.10848983, -0.41619024,  0.10033557,\n",
              "        0.85966784, -0.13698106, -0.1241506 ,  0.06038719,  0.02368295],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LzdT2mUYUrm",
        "colab_type": "text"
      },
      "source": [
        "получение вектора документа"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RSRrFNAYUrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doc_to_vec_fasttext(doc_text):\n",
        "  \"\"\"\n",
        "  creates a vector out of a document using fasttext model\n",
        "  \n",
        "  :param doc_text: string to be transformed into vector\n",
        "  :return: np.array, mean vector of word vectors in provided text\n",
        "  \"\"\"\n",
        "  lemmas = lemmatize(doc_text) # сделали препроцессинг, получили леммы \n",
        "  # создаем маски для векторов \n",
        "  lemmas_vectors = np.zeros((len(lemmas), model.vector_size))\n",
        "  doc_vec = np.zeros((model.vector_size,))\n",
        "  # если слово есть в модели, берем его вектор\n",
        "  for idx, lemma in enumerate(lemmas):\n",
        "    if lemma in model.vocab: # word in vocab\n",
        "      try:\n",
        "        lemmas_vectors[idx] = model[lemma]\n",
        "      except AttributeError as e: # word in vocab but not in model\n",
        "        print(e)\n",
        "  if lemmas_vectors.shape[0] is not 0:  # проверка на случай, если на вход пришел пустой массив\n",
        "    doc_vec = np.mean(lemmas_vectors, axis=0)\n",
        "    return doc_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jekvmOHTnYt5",
        "colab_type": "code",
        "outputId": "f9b367e1-b189-4ffe-a0d3-03588ba3d39f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "doc_to_vec_fasttext('я полны дебил то делать?')[10:20]"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.09096584,  0.00550343, -0.039503  , -0.1670939 , -0.13090189,\n",
              "       -0.03988818,  0.07535863,  0.04886302,  0.06259536, -0.12735136])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrpLIg2kMZmq",
        "colab_type": "text"
      },
      "source": [
        "I see no point in writing a separate fasttext indexing function as it would be trivial (below)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7jqfh8Enq0o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "459aec6a-7d0d-4e07-c16a-67e71457352b"
      },
      "source": [
        "print('fasttext')\n",
        "start = time()\n",
        "fasttext_doc2vec_corpus = []\n",
        "for doc in train_texts:\n",
        "  fasttext_doc2vec_corpus.append(doc_to_vec_fasttext(doc))\n",
        "time_fasttext = time() - start\n",
        "print(f'TIME: {str(time_fasttext)}')\n",
        "print(f' example of text: {train_texts[0]},\\n first 10 values from its vector:\\n {fasttext_doc2vec_corpus[0][:10]}')"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fasttext\n",
            "TIME: 4.307932376861572\n",
            " example of text: что произойдет, если правительство Индии украдет кохинор кох-и-ноор-алмаз назад,\n",
            " first 10 values from its vector:\n",
            " [ 0.12532789  0.04991495  0.18384925  0.03815691 -0.00037019  0.0517497\n",
            "  0.01790214 -0.11584419 -0.02457854  0.06156652]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZkzFck0_kCz",
        "colab_type": "text"
      },
      "source": [
        "**ELMO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EfcsQ4B_l1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sys.path.append('/content/content')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XJAd1AmYJpW",
        "colab_type": "text"
      },
      "source": [
        "model-specific imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQJ8p8V2COi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from elmo_helpers import tokenize, get_elmo_vectors, load_elmo_embeddings\n",
        "\n",
        "tf.reset_default_graph()\n",
        "elmo_path = 'content/elmo'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQlVwTqWFPjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_vec(vect, sent):\n",
        "    \"\"\"\n",
        "    deletes empty vectors at the end of a sentence\n",
        "\n",
        "    :param vect: np.array, vector from ELMo\n",
        "    :param sent: list of str, tokenized sentence\n",
        "    :return: np.array\n",
        "    \"\"\"\n",
        "    cropped_vector = vect[:len(sent), :]\n",
        "    cropped_vector = np.mean(cropped_vector, axis=0)\n",
        "    return cropped_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH6jfAvPEjxQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d501410-f605-4768-8d14-97a68ca36db1"
      },
      "source": [
        "tokenize('я ничего не могу сделать')"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['я', 'ничего', 'не', 'могу', 'сделать']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1HwU0BqDQ0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "0e3dc9bb-6fd5-4b09-9295-1b7bb814a7c4"
      },
      "source": [
        "batcher, sentence_character_ids, elmo_sentence_input = load_elmo_embeddings(elmo_path)"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fdb1e324ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fdb1e324ac8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fdb1e324ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fdb1e324ac8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fdb1e324898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fdb1e324898>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fdb1e324898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fdb1e324898>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fdb1c991c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fdb1c991c88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fdb1c991c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fdb1c991c88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fdb1c6105f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fdb1c6105f8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fdb1c6105f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fdb1c6105f8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkHDgvZIJfqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doc_to_vec_elmo(doc_text):\n",
        "  \"\"\" \n",
        "  creates a vector out of a document using fasttext model\n",
        "  \n",
        "  :param doc_text: string to be transformed into vector\n",
        "  :param batcher, sentence_character_ids, elmo_sentence_input: ELMo model\n",
        "  :return: np.array, mean vector of word vectors in provided text\n",
        "    \"\"\"\n",
        "  tokens = [tokenize(doc_text)]\n",
        "  with tf.Session() as sess:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      vector = crop_vec(get_elmo_vectors(sess, tokens, batcher,\n",
        "                                         sentence_character_ids,\n",
        "                                         elmo_sentence_input)[0], tokens[0])\n",
        "  return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZpkB8C9J6cO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b1bc598c-c7f8-4da7-ec27-234c4006d0e0"
      },
      "source": [
        "doc_to_vec_elmo('тяжело очень мне')"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentences in this batch: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.2806621 ,  0.22832812,  0.54048353, ..., -0.4438243 ,\n",
              "       -0.13787274,  0.08398611], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stP9q2y0FT2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def elmo_indexing(tokenized, batcher, sentence_character_ids, elmo_sentence_input):\n",
        "    \"\"\" \n",
        "    indexing texts using ELMo\n",
        "    :param tokenized: list of lists of str, tokenized documents from the corpus\n",
        "    :param batcher, sentence_character_ids, elmo_sentence_input: ELMo model\n",
        "\n",
        "    :return: matrix of document vectors\n",
        "    \"\"\"\n",
        "    with tf.Session() as sess:\n",
        "        # It is necessary to initialize variables once before running inference.\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        indexed = []\n",
        "        for i in range(200, len(tokenized)+1, 200):\n",
        "            sentences = tokenized[i-200 : i]\n",
        "            elmo_vectors = get_elmo_vectors(\n",
        "                sess, sentences, batcher, sentence_character_ids, elmo_sentence_input)\n",
        "\n",
        "            for vect, sent in zip(elmo_vectors, sentences):\n",
        "                cropped_vector = crop_vec(vect, sent)\n",
        "                indexed.append(cropped_vector)\n",
        "    return indexed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvb0-HttLTtR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "086f0eae-973e-46de-d89c-b4898161bfa0"
      },
      "source": [
        "start = time()\n",
        "elmo_doc2vec_corpus = elmo_indexing([tokenize(q) for q in train_texts], batcher, sentence_character_ids, elmo_sentence_input)\n",
        "print('elmo')\n",
        "time_elmo = time() - start\n",
        "print(f'TIME: {str(time_elmo)}')\n",
        "print(f' example of text: {train_texts[0]},\\n first 10 values from its vector:\\n {elmo_doc2vec_corpus[0][:10]}')"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentences in this batch: 200\n",
            "Sentences in this batch: 200\n",
            "Sentences in this batch: 200\n",
            "Sentences in this batch: 200\n",
            "Sentences in this batch: 200\n",
            "Sentences in this batch: 200\n",
            "Sentences in this batch: 200\n",
            "Sentences in this batch: 200\n",
            "Sentences in this batch: 200\n",
            "Sentences in this batch: 200\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "elmo\n",
            "TIME: 11.883128643035889\n",
            " example of text: что произойдет, если правительство Индии украдет кохинор кох-и-ноор-алмаз назад,\n",
            " first 10 values from its vector:\n",
            " [-0.26944843 -0.1237908   0.34158683  0.16239585 -0.635508    0.08462887\n",
            "  0.3105996  -0.17464824  0.07552434  0.3336463 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0LoP-oIWq7D",
        "colab_type": "text"
      },
      "source": [
        "**BM25 (lemmatized)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwk-RtPiW9eB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 2.0\n",
        "N = trained_size\n",
        "vectorizer = CountVectorizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FCZArxYfkID",
        "colab_type": "text"
      },
      "source": [
        "define mean text length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgmw7JIpfkIO",
        "colab_type": "code",
        "outputId": "b105fa9a-0ff6-49f8-e8a5-4fdf0d721a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lens = [len(text.split()) for text in train_texts]\n",
        "avgdl = sum(lens)/N\n",
        "avgdl"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.5675"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1blnKTUWfkIU",
        "colab_type": "text"
      },
      "source": [
        "precompute a count matrix\n",
        "<br> rows - documents\n",
        "<br> columns - words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoukPTqSfkIY",
        "colab_type": "code",
        "outputId": "1f3c7795-ea4c-4c3e-d5bb-d4a3d9980d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X = vectorizer.fit_transform([' '.join(lemmatize(q)) for q in train_texts])\n",
        "count_matrix = X.toarray()\n",
        "count_matrix.shape"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 4489)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM8YeQIXfkIk",
        "colab_type": "text"
      },
      "source": [
        "precompute tfs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DP_yWLVfkIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_matrix = count_matrix / np.array(lens).reshape((-1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CR09SQ1fkIt",
        "colab_type": "text"
      },
      "source": [
        "get a vocabulary that has the same indexation as the rows of the count matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZggsEtffkIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary = vectorizer.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNVYbZeNZYh4",
        "colab_type": "text"
      },
      "source": [
        "get idfs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUrEdnygZPYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_n_docs = np.count_nonzero(count_matrix, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86TdrZIaZb3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def IDF_modified(word):\n",
        "    word_id = vocabulary.index(word)\n",
        "    n = in_n_docs[word_id]\n",
        "    return log((N - n + 0.5) / (n + 0.5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YxyLOC5Zm6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idfs = [IDF_modified(word) for word in vocabulary]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFdEonNsaICE",
        "colab_type": "text"
      },
      "source": [
        "compute bm25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sis6eedCaDTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def modify_tf(tf_value, doc_index, b=0.75):\n",
        "    l = lens[doc_index]\n",
        "    return (tf_value * (k + 1.0))/(tf_value + k * (1.0 - b + b * (l/avgdl)))\n",
        "\n",
        "def modify_tf_matrix(tf_matrix, b=0.75): \n",
        "    enumed =  np.ndenumerate(tf_matrix)\n",
        "    for i, tf_value in enumed:\n",
        "        doc_index = i[0]\n",
        "        tf_matrix[i] = modify_tf(tf_value, doc_index, b)\n",
        "    return tf_matrix*idfs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5yW-IUfab8S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4dd76c2c-16b7-4453-9454-78a050fad0de"
      },
      "source": [
        "start = time()\n",
        "modified_tf_matrix = modify_tf_matrix(tf_matrix)\n",
        "print('bm25')\n",
        "time_bm25_lemm = time() - start\n",
        "print(f'TIME Indexing: {str(time_bm25_lemm)}')"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bm25\n",
            "TIME Indexing: 12.373842000961304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHuqByZoaMyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bm25_vector(query):\n",
        "    vector = np.array(vectorizer.transform([' '.join(lemmatize(query))]).todense())[0]\n",
        "    binary_vector = np.vectorize(lambda x: 1.0 if x != 0.0 else 0.0)(vector) ## neutralizes duplictes in the query (non-lineraity)\n",
        "    return modified_tf_matrix.dot(binary_vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAWJy_8maZPK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "05a7b580-2fe2-46f5-de39-8b6bfdb0ab37"
      },
      "source": [
        "sum(bm25_vector('в воде'))"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.132411112541131"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKEFdHA8cFcU",
        "colab_type": "text"
      },
      "source": [
        "**indexation comparison**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n-vch4sOaPb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6bc83247-5762-4663-bdfe-96ced16d0664"
      },
      "source": [
        "print(f'let us compare indexation times on trained size {trained_size}:\\n fasttext: {time_fasttext} sec\\n elmo: {time_elmo} sec,\\n  bm25: {time_bm25_lemm} sec ')"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "let us compare indexation times on trained size 2000:\n",
            " fasttext: 4.296413898468018 sec\n",
            " elmo: 11.95018744468689 sec,\n",
            "  bm25: 12.373842000961304 sec \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECpDavvvYUrw",
        "colab_type": "text"
      },
      "source": [
        "### __Задача 2__:    \n",
        "Выведите качество поиска для каждой модели +  BM25 для сравнения\n",
        "\n",
        "Качество оцениваем так же, как в прошлом задании:\n",
        "    - если в топ-5 результатов выдачи попал хоть один релевантный документ, выдача точная\n",
        "    - если в топ-5 нет ни одного релеватного документа, выдача получает 0\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htMC6D8DYUrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search_fasttext(query, n=trained_size): \n",
        "    \"\"\"\n",
        "    searches a given query in fasttext model, returns top n results, by default n = all found (the legth of collection)\n",
        "    \n",
        "    :param query: text to search for, string\n",
        "    :param n: how many most relevant results to return, int\n",
        "    :return: list of triplets (document id, document text, cos_sim to document text)\n",
        "    \"\"\"\n",
        "    cos_sim_relevance = [cos_sim(doc_to_vec_fasttext(query), doc) for doc in fasttext_doc2vec_corpus]\n",
        "    relevance_sorted_document_ids_top_n = enum_sort(cos_sim_relevance)[:n]\n",
        "    return [(index, np.array(train_texts)[index], cos_sim_relevance[index]) for index in relevance_sorted_document_ids_top_n]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhF4jM_SNBYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search_elmo(query, n=trained_size): \n",
        "    \"\"\"\n",
        "    searches a given query in elmo model, returns top n results, by default n = all found (the legth of collection)\n",
        "    \n",
        "    :param query: text to search for, string\n",
        "    :param n: how many most relevant results to return, int\n",
        "    :return: list of triplets (document id, document text, cos_sim to document text)\n",
        "    \"\"\"\n",
        "    query_vec = doc_to_vec_elmo(query)\n",
        "    cos_sim_relevance = [cos_sim(query_vec, doc_vec) for doc_vec in elmo_doc2vec_corpus]\n",
        "    relevance_sorted_document_ids_top_n = enum_sort(cos_sim_relevance)[:n]\n",
        "    return [(index, np.array(train_texts)[index], cos_sim_relevance[index]) for index in relevance_sorted_document_ids_top_n]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RPemc5pcfKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search_bm25(query, n=trained_size): \n",
        "    \"\"\"\n",
        "    searches a given query in bm25 model, returns top n results, by default n = all found (the legth of collection)\n",
        "    \n",
        "    :param query: text to search for, string\n",
        "    :param n: how many most relevant results to return, int\n",
        "    :return: list of triplets (document id, document text, cos_sim to document text)\n",
        "    \"\"\"\n",
        "    bms = bm25_vector(query)\n",
        "    relevance_sorted_document_ids_top_n = enum_sort(bms)[:n]\n",
        "    return [(index, np.array(train_texts)[index], bms[index]) for index in relevance_sorted_document_ids_top_n]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts_n5yZtVdvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = 'Почему меня девушки не любят?'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU089m2_9WbF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0cd3b05b-ddb9-4317-ee5f-729e14d617a0"
      },
      "source": [
        "search_fasttext(query, n=5)"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(705, 'почему я не хочу разговаривать с кем-то', 0.831970292342526),\n",
              " (644, 'как я могу быть геем и не люблю гей-секс', 0.8242458474426835),\n",
              " (1486,\n",
              "  'как я могу сказать, действительно ли моя девушка любит меня?',\n",
              "  0.8085746678015144),\n",
              " (1595,\n",
              "  'почему я боюсь рассказать своему парню, что я его люблю',\n",
              "  0.8055456291466281),\n",
              " (392,\n",
              "  'почему люди ненавидят шарух хана и почему люди чувствуют, что он не индийский',\n",
              "  0.7867779928670644)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odf0DW2wVX2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "0c91c9d5-15fe-405a-8fe6-64593ad2d8bd"
      },
      "source": [
        "search_elmo(query, n=5)"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentences in this batch: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1662,\n",
              "  'что я должен делать, если моя подруга, которая обещала мне, что она никогда не покинет меня и не полюбит меня больше, чем моя мама оставила меня для кого-то другого',\n",
              "  0.82276833),\n",
              " (1595, 'почему я боюсь рассказать своему парню, что я его люблю', 0.81353277),\n",
              " (1091, 'почему некоторые люди любят давать минеты', 0.7977652),\n",
              " (523, 'почему моя собака укусила меня', 0.79703826),\n",
              " (1486,\n",
              "  'как я могу сказать, действительно ли моя девушка любит меня?',\n",
              "  0.786887)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1abWbnNjdIil",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8acb36dc-b57c-4ffe-d939-303e7dec6252"
      },
      "source": [
        "search_bm25(query, n=5)"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(368, 'девушки любят черных парней без бороды', 7.573496882335712),\n",
              " (1101, 'почему сербы любят россиян', 6.689078862794888),\n",
              " (590, 'как девушка получает парня', 6.59057109708373),\n",
              " (257, 'почему не большинство индусов едят не-овощные', 3.9836102148059087),\n",
              " (418, 'почему не должен я смотреть порно', 3.9836102148059087)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq1RNdKn-bQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_q1_by_id(q1_index, model_name='fasttext'):\n",
        "  \"\"\"\n",
        "  tests whether for a given question from quora question pairs relevant answer is in top five responses\n",
        "  \n",
        "  :param q1_index: int, question id from quora question pairs dataset\n",
        "  :param model_name: string {'fasttext', 'elmo', 'bm25'}, 'fasttext' by default, which model to use for searching\n",
        "  :return: 0 or 1, boolean accuracy\n",
        "  \"\"\"\n",
        "  if model_name == 'fasttext':\n",
        "    search = search_fasttext    \n",
        "  elif model_name == 'elmo':  \n",
        "    search = search_elmo\n",
        "  elif model_name == 'bm25':\n",
        "    search = search_bm25\n",
        "  else:\n",
        "    raise ValueError(f'model_name must be ether \"fasttext\", \"elmo\", or \"bm25\", but {model_name} was received')\n",
        "  top_5_ids = [i for i, text, bm_25 in search(questions.iloc[q1_index]['question1'], n=5)]\n",
        "  return 1.0 if q1_index in top_5_ids else 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_F6fdvy-VpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_over_multiple_questions(testsize=50, model_name='fasttext'):\n",
        "  \"\"\"\n",
        "  tests search accuracy over a given number of questions form quora question pairs dataset\n",
        "  \n",
        "  :param testsize: int, 50 by default, number of questions to test over,\n",
        "  :param model: string {'fasttext', 'elmo', 'bm25'}, 'fasttext' by default, which model to use for searching\n",
        "  :return: float, mean accuracy over testsize number of questions\n",
        "  \"\"\"\n",
        "  if model_name == 'fasttext':\n",
        "    print('searching fasttext')   \n",
        "  elif model_name == 'elmo':  \n",
        "    print('searching elmo')\n",
        "  elif model_name == 'bm25':\n",
        "    print('searching bm25')\n",
        "  testable = questions[(questions['is_duplicate'] == 1)&(questions.index < trained_size)]\n",
        "  if testsize < len(testable):\n",
        "      testable = testable[:testsize]\n",
        "      print('testing on', testsize, 'questions')\n",
        "  else:\n",
        "      print('testing on', len(testable), 'questions')\n",
        "  hit_count = 0.0\n",
        "  for index in testable.index:\n",
        "      hit_count += test_q1_by_id(index, model_name)\n",
        "  return hit_count/len(testable.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ7NgyfHd6uM",
        "colab_type": "text"
      },
      "source": [
        "**Let us comare accuracies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5dzzoTXkZ--",
        "colab_type": "text"
      },
      "source": [
        "no difference on 20 questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLIbzoRkd5VV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "349d6252-36f7-4d09-ac51-364cd5fa094a"
      },
      "source": [
        "n = 20\n",
        "comparison_20 = []\n",
        "for model_name in ['fasttext', 'elmo', 'bm25']:\n",
        "  start = time()\n",
        "  acc = test_over_multiple_questions(testsize=n, model_name=model_name)\n",
        "  search_time = time() - start\n",
        "  comparison_20.append((model_name, acc, search_time))"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "searching fasttext\n",
            "testing on 20 questions\n",
            "searching elmo\n",
            "testing on 20 questions\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "searching bm25\n",
            "testing on 20 questions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4BgCBoLXNA9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "a713aae2-2dba-4ee1-c15e-9d1740bc373e"
      },
      "source": [
        "for model_name, accuracy, search_time in comparison_20:\n",
        "  print(f'model: {model_name},\\naccuracy: {accuracy},\\nsearch time: {search_time}')"
      ],
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model: fasttext,\n",
            "accuracy: 0.7,\n",
            "search time: 84.62671709060669\n",
            "model: elmo,\n",
            "accuracy: 0.7,\n",
            "search time: 44.6616473197937\n",
            "model: bm25,\n",
            "accuracy: 0.7,\n",
            "search time: 0.47026586532592773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX_ulkvekjNz",
        "colab_type": "text"
      },
      "source": [
        "let us test 50, 100, 150"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOZFWRHLkDYD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "40d25933-0d4a-46b4-ed92-1649497432d8"
      },
      "source": [
        "test_sized_comparison = {50 : [], 100 : [], 150 : []}\n",
        "for n in test_sized_comparison:\n",
        "  comparison = []\n",
        "  for model_name in ['fasttext', 'elmo', 'bm25']:\n",
        "    start = time()\n",
        "    acc = test_over_multiple_questions(testsize=n, model_name=model_name)\n",
        "    search_time = time() - start\n",
        "    comparison.append((model_name, acc, search_time))\n",
        "  test_sized_comparison[n] = comparison"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "searching fasttext\n",
            "testing on 50 questions\n",
            "searching elmo\n",
            "testing on 50 questions\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "searching bm25\n",
            "testing on 50 questions\n",
            "searching fasttext\n",
            "testing on 100 questions\n",
            "searching elmo\n",
            "testing on 100 questions\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "searching bm25\n",
            "testing on 100 questions\n",
            "searching fasttext\n",
            "testing on 150 questions\n",
            "searching elmo\n",
            "testing on 150 questions\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n",
            "Sentences in this batch: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "searching bm25\n",
            "testing on 150 questions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFiT01_9l-qt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "6edcf10e-7739-40cc-8a08-4078c814a142"
      },
      "source": [
        "for n in test_sized_comparison:\n",
        "  print('\\n\\n')\n",
        "  print(f'test on {n} questions')\n",
        "  for model_name, accuracy, search_time in comparison:\n",
        "    print(f'model: {model_name},\\naccuracy: {accuracy},\\nsearch time: {search_time}')\n",
        "    print()"
      ],
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "test on 50 questions\n",
            "model: fasttext,\n",
            "accuracy: 0.7866666666666666,\n",
            "search time: 578.7353513240814\n",
            "\n",
            "model: elmo,\n",
            "accuracy: 0.74,\n",
            "search time: 336.6040380001068\n",
            "\n",
            "model: bm25,\n",
            "accuracy: 0.68,\n",
            "search time: 3.2782135009765625\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "test on 100 questions\n",
            "model: fasttext,\n",
            "accuracy: 0.7866666666666666,\n",
            "search time: 578.7353513240814\n",
            "\n",
            "model: elmo,\n",
            "accuracy: 0.74,\n",
            "search time: 336.6040380001068\n",
            "\n",
            "model: bm25,\n",
            "accuracy: 0.68,\n",
            "search time: 3.2782135009765625\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "test on 150 questions\n",
            "model: fasttext,\n",
            "accuracy: 0.7866666666666666,\n",
            "search time: 578.7353513240814\n",
            "\n",
            "model: elmo,\n",
            "accuracy: 0.74,\n",
            "search time: 336.6040380001068\n",
            "\n",
            "model: bm25,\n",
            "accuracy: 0.68,\n",
            "search time: 3.2782135009765625\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}