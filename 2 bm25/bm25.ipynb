{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Объяснение, почему всё так грустно и медленно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4 Gb\n",
      "available 1 Gb\n",
      "percent 67.8%\n",
      "used 2 Gb\n",
      "free 1 Gb\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "env_info = dict(psutil.virtual_memory()._asdict())\n",
    "for key in env_info:\n",
    "    if key != 'percent':\n",
    "        print(key, str(env_info[key] // 1000000000), 'Gb')\n",
    "    else:\n",
    "        print(key, str(env_info[key])+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 2  BM5    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from math import log\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2.0\n",
    "b = 0.75\n",
    "trained_size = 1000\n",
    "N = trained_size*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = Mystem()\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__important data-independent functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enum_sort(arr): # takes a list and returns a list of ids in the decreasing order of the values from the input\n",
    "    return [x[0] for x in sorted(enumerate(arr), key=lambda x:x[1], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    return [morph.lemmatize(token)[0] for token in nltk.word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, lemm=False):\n",
    "    if lemm:\n",
    "        words = lemmatize(text)\n",
    "    else:\n",
    "        words = nltk.word_tokenize(text)\n",
    "    query_modified = list(set(words).intersection(set(vocabulary)))  \n",
    "    return query_modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-compute data-dependednt constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_csv('quora_question_pairs_rus.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Какова история кохинор кох-и-ноор-бриллиант</td>\n",
       "      <td>что произойдет, если правительство Индии украд...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>как я могу увеличить скорость моего интернет-с...</td>\n",
       "      <td>как повысить скорость интернета путем взлома ч...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>почему я мысленно очень одинок, как я могу это...</td>\n",
       "      <td>найти остаток, когда математика 23 ^ 24 матема...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>которые растворяют в воде быстро сахарную соль...</td>\n",
       "      <td>какая рыба выживет в соленой воде</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>астрология: я - луна-колпачок из козерога и кр...</td>\n",
       "      <td>Я тройная луна-козерог и восхождение в козерог...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0        Какова история кохинор кох-и-ноор-бриллиант   \n",
       "1  как я могу увеличить скорость моего интернет-с...   \n",
       "2  почему я мысленно очень одинок, как я могу это...   \n",
       "3  которые растворяют в воде быстро сахарную соль...   \n",
       "4  астрология: я - луна-колпачок из козерога и кр...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  что произойдет, если правительство Индии украд...             0  \n",
       "1  как повысить скорость интернета путем взлома ч...             0  \n",
       "2  найти остаток, когда математика 23 ^ 24 матема...             0  \n",
       "3                  какая рыба выживет в соленой воде             0  \n",
       "4  Я тройная луна-козерог и восхождение в козерог...             1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__only some texts will be used, a part defined by trained_size constant above__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = questions[:trained_size]['question1'].tolist() + questions[:trained_size]['question2'].tolist()\n",
    "## train_texts = [' '.join(lemmatize(text)) for text in train_texts] ## адово долго!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lemmatized.json', 'w') as f:\n",
    "    f.write(json.dumps(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lemmatized.json', 'r') as f:\n",
    "    train_texts = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__define mean text length__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.201"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = [len(text.split()) for text in train_texts]\n",
    "avgdl = sum(lens)/N\n",
    "avgdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__precompute a count matrix__\n",
    "<br> rows - documents\n",
    "<br> columns - words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 5905)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(train_texts)\n",
    "count_matrix = X.toarray()\n",
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__precompute tfs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_matrix = count_matrix / np.array(lens).reshape((-1, 1))\n",
    "tf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get a vocabulary that has the same indexation as the rows of the count matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['войной',\n",
       " 'войну',\n",
       " 'войны',\n",
       " 'войска',\n",
       " 'войти',\n",
       " 'вокализм',\n",
       " 'вокруг',\n",
       " 'волнение',\n",
       " 'волноваться',\n",
       " 'волнует']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = vectorizer.get_feature_names()\n",
    "vocabulary[1030:1040]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__get idfs__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a list of number of docs with a given word for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  5, 11, ...,  1,  1,  1], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_n_docs = np.count_nonzero(count_matrix, axis=0)\n",
    "in_n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF_modified(word):\n",
    "    word_id = vocabulary.index(word)\n",
    "    n = in_n_docs[word_id]\n",
    "    return log((N - n + 0.5) / (n + 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.6838614462772235"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDF_modified('воде')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.195187320178709,\n",
       " 7.195187320178709,\n",
       " 7.195187320178709,\n",
       " 7.195187320178709,\n",
       " 4.3904097651860585,\n",
       " 6.6838614462772235,\n",
       " 6.6838614462772235,\n",
       " 7.195187320178709,\n",
       " 6.6838614462772235,\n",
       " 7.195187320178709]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs = [IDF_modified(word) for word in vocabulary]\n",
    "idfs[1000:1010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция ранжирования bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обратного индекса есть общепринятая формула для ранжирования *Okapi best match 25* ([Okapi BM25](https://ru.wikipedia.org/wiki/Okapi_BM25)).    \n",
    "Пусть дан запрос $Q$, содержащий слова  $q_1, ... , q_n$, тогда функция BM25 даёт следующую оценку релевантности документа $D$ запросу $Q$:\n",
    "\n",
    "$$ score(D, Q) = \\sum_{i}^{n} \\text{IDF}(q_i)*\\frac{TF(q_i,D)*(k+1)}{TF(q_i,D)+k(1-b+b\\frac{l(d)}{avgdl})} $$ \n",
    "где   \n",
    ">$TF(q_i,D)$ - частота слова $q_i$ в документе $D$      \n",
    "$l(d)$ - длина документа (количество слов в нём)   \n",
    "*avgdl* — средняя длина документа в коллекции    \n",
    "$k$ и $b$ — свободные коэффициенты, обычно их выбирают как $k$=2.0 и $b$=0.75   \n",
    "$$$$\n",
    "$\\text{IDF}(q_i)$ - это модернизированная версия IDF: \n",
    "$$\\text{IDF}(q_i) = \\log\\frac{N-n(q_i)+0.5}{n(q_i)+0.5},$$\n",
    ">> где $N$ - общее количество документов в коллекции   \n",
    "$n(q_i)$ — количество документов, содержащих $q_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement tf part of the formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_tf(tf_value, doc_index):\n",
    "    l = lens[doc_index]\n",
    "    return (tf_value * (k + 1.0))/(tf_value + k * (1.0 - b + b * (l/avgdl)))\n",
    "\n",
    "def modify_tf_matrix(tf_matrix): \n",
    "    enumed =  np.ndenumerate(tf_matrix)\n",
    "    for i, tf_value in enumed:\n",
    "        doc_index = i[0]\n",
    "        tf_matrix[i] = modify_tf(tf_value, doc_index)\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_tf_matrix = modify_tf_matrix(tf_matrix)\n",
    "modified_tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2f884134ad04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'БЛЯТЬ!111 Я ЗАЕБАЛАСЬ ВОороны!22 ебутся в воде !11'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpreprocess_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_query' is not defined"
     ]
    }
   ],
   "source": [
    "q = 'БЛЯТЬ!111 Я ЗАЕБАЛАСЬ ВОороны!22 ебутся в воде !11'\n",
    "preprocess_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Задача 1__:    \n",
    "Напишите два поисковика на *BM25*. Один через подсчет метрики по формуле для каждой пары слово-документ, второй через умножение матрицы на вектор. \n",
    "\n",
    "Сравните время работы поиска на 100к запросах. В качестве корпуса возьмем \n",
    "[Quora question pairs](https://www.kaggle.com/loopdigga/quora-question-pairs-russian).\n",
    "\n",
    "__НА СТА ТЫСЯЧАХ НИКАК НЕ МОГУ, У МЕНЯ ОНО НА ДВУХ ТЫСЯЧАХ (trained_size in constants * 2) МИНУТУ КРУТИТСЯ!!11__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define two bm25 implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### реализуйте эту функцию ранжирования векторно\n",
    "def bm25_vector(query, lemm=False):\n",
    "    vector = np.array(vectorizer.transform([' '.join(preprocess(query, lemm))]).todense())[0]\n",
    "    binary_vector = np.vectorize(lambda x: 1.0 if x != 0.0 else 0.0)(vector) ## neutralizes duplictes in the query (non-lineraity)\n",
    "    idfs_from_query = np.array(idfs)*np.array(binary_vector)\n",
    "    return modified_tf_matrix.dot(idfs_from_query) ## bm25 близость для каждого документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### реализуйте эту функцию ранжирования итеративно\n",
    "def bm25_iter(query, lemm=False):\n",
    "    query_words = preprocess(query, lemm)\n",
    "    relevance = []\n",
    "    for i in range(N):\n",
    "        doc_index = i\n",
    "        doc_bm25 = 0.0\n",
    "        for word in set(query_words): ## set neutralizes duplictes in the query\n",
    "            word_index = vocabulary.index(word)\n",
    "            tf_value = tf_matrix[(doc_index, word_index)]\n",
    "            doc_bm25 += idfs[word_index] * modify_tf(tf_value, doc_index)\n",
    "        relevance.append(doc_bm25)\n",
    "    return relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compare performance__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'если честно, мне кажется, что мой итеративный алгоритм работает очень плохо 11 !!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "TIME non-lemmatized query: 0.02601909637451172\n",
      "2000\n",
      "TIME lemmatized query: 46.89918899536133\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print(len(bm25_vector(query)))\n",
    "print('TIME non-lemmatized query: ' + str(time() - start))\n",
    "start = time()\n",
    "print(len(bm25_vector(query, lemm=True)))\n",
    "print('TIME lemmatized query: ' + str(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "TIME non-lemmatized query: 2.279613971710205\n",
      "2000\n",
      "TIME lemmatized query: 50.14648675918579\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print(len(bm25_iter(query)))\n",
    "print('TIME non-lemmatized query: ' + str(time() - start))\n",
    "start = time()\n",
    "print(len(bm25_iter(query, lemm=True)))\n",
    "print('TIME lemmatized query: ' + str(time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__quod erad demonstrandum!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Задача 2__:    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите 10 первых результатов и их близость по метрике BM25 по запросу *рождественские каникулы* на нашем корпусе  Quora question pairs. \n",
    "\n",
    "__выведу только поиск на первых 2000 (trained_size in constants * 2) документов, должно работать вообще, но моё железо не тянет__<br>\n",
    "__по *рождественским каникулам* в первых 2000 доков ничего нет, так что на примере другой query__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: война с водой \n",
      "\n",
      "relevance rank: 1\n",
      "document: мировая война iii неизбежна\n",
      "bm_25 =  2.859286094328463 \n",
      "\n",
      "relevance rank: 2\n",
      "document: как неизбежно мировая война iii\n",
      "bm_25 =  2.1167909836038605 \n",
      "\n",
      "relevance rank: 3\n",
      "document: 3-я мировая война неизбежна, чем ожидалось\n",
      "bm_25 =  1.6249035320650158 \n",
      "\n",
      "relevance rank: 4\n",
      "document: почему тауфины с морской водой импортированы в Австралию\n",
      "bm_25 =  1.3986039618245336 \n",
      "\n",
      "relevance rank: 5\n",
      "document: может ли мировая война 3 когда-либо иметь место\n",
      "bm_25 =  1.0390320082083848 \n",
      "\n",
      "relevance rank: 6\n",
      "document: будет ли ядерная война между Индией и Пакистаном\n",
      "bm_25 =  1.0390320082083848 \n",
      "\n",
      "relevance rank: 7\n",
      "document: если будет война между Индией и Пакистаном, которая выиграет\n",
      "bm_25 =  0.8573098496307885 \n",
      "\n",
      "relevance rank: 8\n",
      "document: кто победит, если начнется война между Индией и Пакистаном\n",
      "bm_25 =  0.8573098496307885 \n",
      "\n",
      "relevance rank: 9\n",
      "document: должна ли быть война между Индией и Пакистаном для кашмира\n",
      "bm_25 =  0.7190225447873438 \n",
      "\n",
      "relevance rank: 10\n",
      "document: действительно ли будет какая-то война между Индией и Пакистаном над нападением на ури, каковы будут ее последствия\n",
      "bm_25 =  0.28325023891027457 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'война с водой'\n",
    "n = 10\n",
    "bms = bm25_vector(query)\n",
    "ids_top_n = enum_sort(bms)[:n]\n",
    "print('query:', query, '\\n')\n",
    "for i in range(n):\n",
    "    print('relevance rank:', i+1)\n",
    "    print('document:', np.array(train_texts)[ids_top_ten[i]])\n",
    "    print('bm_25 = ', bms[ids_top_ten[i]], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Задача 3__:    \n",
    "\n",
    "Посчитайте точность поиска при \n",
    "1. BM25, b=0.75 \n",
    "2. BM15, b=0 \n",
    "3. BM11, b=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
